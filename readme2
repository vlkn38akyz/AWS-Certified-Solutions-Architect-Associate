1-Your app uses AWS Cognito Identity for authentication and stores user profiles in a User Pool. 
To expand the availability and ease of signing in to the app, your team is requesting advice on allowing 
the use of OpenID Connect (OIDC) identity providers as additional means of authenticating users and saving
the user profile information. What is your recommendation on OIDC identity providers?

This is supported, along with social and SAML based identity providers.

This is not supported, only social identity providers can be integrated into User Pools

If you want OIDC identity providers, then you must include SAML and socialbased support as well

It’s too much effort to add non-Cognito authenticated user information to a User Pool
0/1 point
Explanation: Option A is correct. OpenID Connect (OIDC) identity providers (IdPs) (like Salesforce or Ping Identity) are supported in Cognito,
along with social and SAML based identity providers. You can add an OIDC IdP to your user pool in the AWS Management Console, with the AWS CLI, 
or by using the user pool API method CreateIdentityProvider. Option B is incorrect. Cognito supports more than just social identity providers,
including OIDC, SAML, and its own identity pools. Option C is incorrect. You can add any combination of federated types, you don’t have to add them all. 
Option D is incorrect. While there is additional coding to develop this, the effort is most likely not too great to add the feature.


2-Your company currently has a web distribution hosted using the AWS CloudFront service. The IT Security department has confirmed that the application using this web distribution now falls under the scope of PCI compliance. What are the possible ways to meet the requirements? (SELECT TWO)

Enable CloudFront access logs.

Enable Cache in CloudFront.

Capture requests that are sent to the CloudFront API.

Enable VPC Flow Logs
1/2 points
Explanation: Correct Answer – A and C AWS Documentation mentions the following: If you run PCI or HIPAA-compliant workloads based on the AWS Shared Responsibility Model, we recommend that you log your CloudFront usage data for the last 365 days for future auditing purposes. To log usage data, you can do the following: -Enable CloudFront access logs.-Capture requests that are sent to the CloudFront API. Option B is incorrect. It helps to reduce latency. Option D is incorrect. VPC flow logs capture information about the IP traffic going to and from network interfaces in a VPC but not for CloudFront.

3-A company has set up an application in AWS that interacts with DynamoDB. It is required that when an item is modified in a DynamoDB table, immediate entry is made to the associating application. How can this be accomplished? (SELECT TWO)

Setup CloudWatch to monitor the DynamoDB table for changes. Then trigger a Lambda function to send the changes to the application.

Setup CloudWatch logs to monitor the DynamoDB table for changes. Then trigger AWS SQS to send the changes to the application.

Use DynamoDB streams to monitor the changes to the DynamoDB table.

Trigger a lambda function to make an associated entry in the application as soon as the DynamoDB streams are modified.
1/2 points
Expalanation: Correct Answer – C and D When you enable DynamoDB Streams on a table, you can associate the stream ARN with a Lambda function that you write. Immediately after an item in the table is modified, a new record appears in the table&#39;s stream. AWS Lambda polls the stream and invokes your Lambda function synchronously when it detects new stream records. Since our requirement is to have an immediate entry made to an application in case an item in the DynamoDB table is modified, a lambda function is also required. Let us try to analyze this with an example: Consider a mobile gaming app that writes to a GamesScores table. Whenever the top score of the Game Scores table is updated, a corresponding stream record is written to the table&#39;s stream. This event could then trigger a Lambda function that posts a Congratulatory message on a Social media network handle. DynamoDB streams can be used to monitor the changes to a DynamoDB table.

4-As a Solutions Architect for a multinational organization having more than 150000 employees, management has decided to implement a real-time analysis for their employees' time spent in offices across the globe. You are tasked to design an architecture that will receive the inputs from 10000+ sensors with swipe machine sending in and out data across the globe, each sending 20KB data every 5 Seconds in JSON format. The application will process and analyze the data and upload the results to dashboards in realtime. Other application requirements will include the ability to apply real-time analytics on the captured data, processing of captured data will be parallel and durable, the application must be scalable as per the requirement as the load varies and new sensors are added or removed at various facilities. The analytic processing results are stored in a persistent data storage for data mining. What combination of AWS services would be used for the above scenario?

Use EMR to copy the data coming from Swipe machines into DynamoDB and make it available for analytics

Use Amazon Kinesis Streams to ingest the Swipe data coming from sensors, Custom Kinesis Streams Applications to analyze the data and then move analytics outcomes to RedShift using AWS EMR

Use SQS to receive the data coming from sensors, Kinesis Firehose to analyze the data from SQS, then save the results to a Multi-AZ RDS instance

Use Amazon Kinesis Streams to ingest the sensors’ data, custom Kinesis Streams applications to analyze the data, and move analytics outcomes to RDS using AWS EMR
0/1 point
Expalanation: Option A is incorrect. EMR is not for receiving the real-time data from thousands of sources, EMR is mainly used for Hadoop ecosystem-based data used for Big data analysis. Option B is correct as the Amazon Kinesis streams are used to read the data from thousands of sources like social media, survey-based data, etc. The Kinesis streams can be used to analyze the data and can feed it using AWS EMR to the analytics-based database like RedShift which works on OLAP. Option C is incorrect, SQS cannot be used to read the real-time data from thousands of sources. Besides, the Kinesis Firehose is used to ship the data to other AWS service, not for the analysis. And finally, RDS is again an OLTP based database. Option D is incorrect as the AWS EMR can read large amounts of data, however, RDS is a transactional database that works based on the OLTP. Thus, it cannot store the analytical data.

5-You have designed an application that uses AWS resources, such as S3 to operate and store users’ documents. You currently use Cognito identity pools and user pools. To increase usage and ease of signing up, you decide that adding social identity federation is the best path forward. How would you differentiate the Cognito identity pool and the federated identity providers (e.g. Google)?

They are the same and just called different things

First, you sign-in via Cognito then through a federated site, like Google

Federated identity providers and identity pools are used to authenticate services

Sign-in via AWS Cognito User Pool and sign-in via AWS Cognito Identity Pool are independent of one another
1/1 point
Expalanation: Correct Answer – D Option D is correct. Sign-in through a third party (federation) is available in Amazon Cognito user pools. This feature is independent of the federation through Amazon Cognito identity pools (federated identities). Option A is incorrect. Cognito identity pool and the federated identity providers are separate, independent authentication methods. Option B is incorrect. Only one log-in event is needed, not two. Option C is incorrect. Identity providers authenticate users, not authenticate services.

6- You currently have your EC2 instances running in multiple availability zones. You have a NAT gateway defined for your private instances and you want to make this highly available. How could this be accomplished?

Create another NAT Gateway and place it behind an ELB.

Create a NAT Gateway in another Availability Zone.

Create a NAT Gateway in another region.

Use Auto Scaling groups to scale the NAT Gateway.
0/1 point
Explanation: Correct Answer - B AWS Documentation mentions the following: If you have resources in multiple Availability Zones and they share one NAT Gateway, in the event that the NAT Gateway’s Availability Zone is down, resources in the other Availability Zones lose internet access. To create an Availability Zone-independent architecture, create a NAT Gateway in each Availability Zone and configure your routing to ensure that resources use the NAT Gateway in the same Availability Zone.

7-Third-party sign-in (Federation) has been implemented in your web application to allow users who need access to AWS resources. Users have been successfully logging in using Google, Facebook, and other third-party credentials. Suddenly, their access to some AWS resources has been restricted. What is the most likely cause of the restricted use of AWS resources?

IAM policies for resources were changed, thereby restricting access to AWS resources

Federation protocols are used to authorize services and need to be updated

AWS changed the services allowed to be accessed via federated login

The identity providers no longer allow access to AWS services
0/1 point
Expalanation: Correct Answer: A Option A is correct. When IAM policies are changed, they can impact the user experience and services they can connect to. Option B is incorrect. Federation is used to authenticate users, not to authorize services. Option C is incorrect. Federation is used to authenticate users, not to authorize services. Option D is incorrect. The identity providers don’t have the capability to authorize services; they authenticate users.

8-A security audit discovers that one of your RDS MySQL instances is not encrypted. The instance has a Read Replica in the same AWS region which is also not encrypted. You need to fix this issue as soon as possible. What is the proper way to add encryption to the instance and its replica?

Copy a DB snapshot and encrypt the snapshot. Restore a new DB instance from the encrypted snapshot and add a Read Replica.

Encrypt the DB instance. Launch a new Read Replica and the replica is encrypted automatically.

Create a DB snapshot and encrypt the snapshot. Launch a new instance and its Read Replica from the snapshot.

Promote the Read Replica to be a standalone instance and encrypt it. Add a new Read Replica to the standalone instance.
0/1 point
Explanation: Correct Answer – A Existing unencrypted RDS instances and their snapshots cannot be encrypted. Users can only enable encryption for an RDS DB instance when they create it. Option A is CORRECT: Because you can encrypt a copy of an unencrypted DB snapshot. Then the new RDS instance launched from the snapshot and its Read Replica are encrypted. Option B is incorrect: Because you cannot encrypt an unencrypted RDS instance directly. Option C is incorrect: Because you cannot encrypt an unencrypted DB snapshot according to the above reference. Option D is incorrect: Because an unencrypted DB Read Replica cannot be encrypted. The correct method is to launch a new instance from an encrypted DB snapshot.

9-You have an application hosted on AWS consisting of EC2 Instances launched via an Auto Scaling Group. You notice that the EC2 Instances are not scaling on demand. Which checks should be done to ensure that the scaling occurs as expected? (Select 2)

Ensure that the right metrics are being used to trigger the scale-out.

Check your scaling policies to see whether more than one policy is triggered by an event.

Ensure that AutoScaling health checks are being used.

Ensure that you are using Load Balancers.
0/2 points
Explanation: Correct Answer – A and B There could be a number of reasons as mentioned in AWS Documentation but only options A and B are applicable from the given choices. Option A is correct because if your scaling events are not based on the right metrics and do not have the right threshold defined, then the scaling will not occur as you want it to happen. Option B is correct because f two policies are executed at the same time, Amazon EC2 Auto Scaling follows the policy with the greater impact. For example, if you have one policy to add two instances and another policy to add four instances, Amazon EC2 Auto Scaling adds four instances when both policies are triggered at the same time. Option C is incorrect because health checks will help us know the health status of an Auto Scaling instance. It is not a Check if AutoScaling is not working as expected. It is a health check for Instance. Option D is incorrect because AutoScaling can be used without Load Balancer also.

10-A company currently hosts a Redshift cluster in AWS. For security reasons, it should ensure that all traffic from and to the Redshift cluster does not go through the Internet. Which features can be used to fulfill this requirement in an efficient manner?

Enable Amazon Redshift Enhanced VPC Routing.

Create a NAT Gateway to route the traffic.

Create a NAT Instance to route the traffic.

Create a VPN Connection to ensure traffic does not flow through the Internet.
0/1 point
Explanation: Correct Answer - A AWS Documentation mentions the following: When you use Amazon Redshift Enhanced VPC Routing, Amazon Redshift forces all COPY and UNLOAD traffic between your cluster and your data repositories through your Amazon VPC. If Enhanced VPC Routing is not enabled, Amazon Redshift routes traffic through the Internet, including traffic to other services within the AWS network.

11-A Singapore based large Architect firm is using Amazon S3 bucket to save all architecture drawings. This firm works globally & multiple accounts are created within the Singapore region as well in other regions to access AWS resources. Users in all these accounts access the Amazon S3 bucket for architectural drawings. AWS Organisation is created for accounts in the Singapore region. Central IT Teams are managing access to S3 buckets using Service Control Policies with AWS Organisations. While applying SCP to an AWS Organisation which of the following needs to be considered to avoid blocking of legitimate user access?

SCP will block access to Amazon S3 bucket to all accounts within the Singapore region including root users of each account within AWS Organisation as well as access to users outside this region who have access to S3 bucket.

SCP will block access to Amazon S3 bucket to all accounts within the Singapore region including root users of each account within AWS Organisation & not to users outside this region who have access to S3 bucket.

SCP will block access to Amazon S3 bucket to all accounts within the Singapore region excluding root users of each account within AWS Organisation as well as access to users outside this region who have access to S3 bucket.

SCP will block access to Amazon S3 bucket to all accounts within the Singapore region excluding root users of each account within AWS Organisation & not to users outside this region who have access to S3 bucket.
0/1 point
Explanation: Correct Answer – B Service Control Policies will be applied to all users within member accounts including root accounts within each of accounts. These policies are not applied to users who are part of these accounts under Aws Organisations & have permission to access Aws resources. Option A is incorrect as SCP doesn't impact users outside the accounts with AWS Organisations having access to AWS resources. Option C is incorrect as SCP will apply to all accounts within AWS organizations including root accounts of individual accounts in an AWS Organisation. Option D is incorrect as SCP will apply to all accounts within AWS organizations including root accounts of individual accounts in an AWS Organisation.

12-A global content management company is using Amazon Aurora as a database for scaling millions of documents with high throughput. The Development Team has created a new version of the database which needs to be shared with TEST and PRODUCTION accounts within the company which will run their own OLAP queries. The company is using AWS Organisations to manage policies & have consolidated billing across all AWS accounts. Which of the following can be done to share DB clusters with the TEST account?

Enable sharing for Master account of AWS organizations & grant access to TEST account sharing DB cluster from its own account as well as DB shared by Production account.

Enable sharing for member accounts of AWS organizations & grant access to the TEST account sharing DB cluster from its own account.

Enable sharing for Master & member account of AWS organizations & grant access TEST account sharing DB cluster from its own account as well as DB shared by Production account.

Enable sharing for Master account of AWS organizations & grant access to TEST account sharing DB cluster from its own account.
0/1 point
Explanation: Correct Answer – D For sharing AWS Resources with AWS Resource Access Manager, sharing needs to be enabled with the master account of AWS Organisation. Only resources that are owned by the account are shared with other accounts & resources are not re-shared from other accounts. Option A is incorrect as Resources shared by other accounts cannot be reshared to other accounts. Only Own resources can be shared. Option B is incorrect as Resource sharing needs to be enabled for the master account & not member account. Option C is incorrect as Sharing needs to be enabled only for Master Account & not member account within an Organisation.

13- A company has a lot of data hosted on their On-premises infrastructure. Running out of storage space, the company wants a quick win solution using AWS. Which of the following would allow easy extension of their data infrastructure to AWS?

The company could start using Gateway Cached Volumes.

The company could start using Gateway Stored Volumes.

The company could start using the DEEP_ARCHIVE storage class.

The company could start using Amazon Glacier.
0/1 point
Explanation: Correct Answer - A Volume Gateways and Cached Volumes can be used to start storing data in S3. AWS Documentation mentions the following: You store your data in Amazon Simple Storage Service (Amazon S3) and retain a copy of frequently accessed data subsets locally. Cached volumes offer substantial cost savings on primary storage and minimize the need to scale your storage on-premises. You also retain low-latency access to your frequently accessed data.

14-A Large Medical Institute is using a legacy database for saving all its patient details. Due to compatibility issues with the latest software they are planning to migrate this database to AWS cloud infrastructure. This large size database will be using a NoSQL database Amazon DynamoDB in AWS. As an AWS consultant, you need to ensure that all tables of the current legacy database are migrated without a glitch to Amazon DynamoDB. Which of the following is the most cost-effective way of transferring legacy databases to Amazon DynamoDB?

Use AWS DMS with AWS Schema Conversion Tool to save data to Amazon S3 bucket & then upload all data to Amazon DynamoDB.

Use AWS DMS with engine conversion tool to save data to Amazon S3 bucket & then upload all data to Amazon DynamoDB.

Use AWS DMS with engine conversion tool to save data to Amazon EC2 & then upload all data to Amazon DynamoDB.

Use AWS DMS with AWS Schema Conversion Tool to save data to Amazon EC2 instance & then upload all data to Amazon DynamoDB.
0/1 point
Explanation: Correct Answer – A In this case Legacy Database will be converted to Amazon DynamoDB which will be a heterogenous conversion. Using AWS Schema Conversion Tool is best suited for such conversion along with AWS DMS to transfer data from on-premise to AWS. Using Amazon S3 bucket will help to save any amount of data in a most cost-effective way before its uploaded to Amazon DynamoDB. Option B is incorrect as engine conversion tool is best suited for homogeneous database migration, in this case it's heterogeneous database, so using AWS SCT along with AWS DMS is a best option. Option C & D are incorrect as using Amazon S3 bucket is a more cost-effective option than using Amazon EC2 instance.

15-You are planning to use Docker containers on a cluster of EC2 instances. These EC2 instances will be launched in a VPC and will require access to ECR and S3 to download Docker images and other images respectively. Additionally, the EC2 instances require secure connectivity to the ECS control plane. You have created public and private subnets to launch the EC2 instances. What would be helpful to enable secure connectivity and ensure all container orchestration traffic stays within the VPC? (SELECT TWO)

Use AWS PrivateLink to connect to the Amazon S3 buckets for downloading images.

For the instances in the public subnets, use Internet Gateway to access Amazon ECS, ECR, and S3 buckets.

Use a Gateway VPC Endpoint to download images from the S3 bucket.

Use AWS PrivateLink to connect to Amazon ECS for control plane connectivity and ECR for downloading Docker images.

For the instances in the private subnets, use NAT to access Amazon ECS, ECR, and S3.

16-A popular blogging site is planning to save all its data to EFS as a redundancy plan. This database is constantly fetch & updated by client information. You need to ensure that all files saved at EFS using AWS DataSync are validated for data-integrity for each packet. Which of the following will ensure fast transfer for data between on-premise & EFS with data integrity done as per security guidelines?

Enable Verification & perform all data transfer.

Enable verification during initial file transfers & disable it post last data transfer.

Disable verification during initial file transfers & enable it post last data transfer.

Disable Verification & perform all data transfer.
0/1 point
Explanation: Correct Answer – C While transferring a constantly changing database between on-premise servers & EFS using AWS DataSync, data verification can be disabled during data transfer & can be enabled post data transfer for data integrity checks & ensure that all data is properly copied between on-premise servers & EFS. Option A is incorrect as enabling data verification for a constantly changing database will lead to slow transfer of data. Option B is incorrect as Verification needs to be performed post data transfer to ensure all data is properly copied to EFS. Option D is incorrect as Disabling verification will not perform data integrity check on data transfer between on-premise servers & EFS.



Use a Gateway VPC Endpoint to connect to Amazon ECS for control plane connectivity and ECR for downloading Docker images.
2/2 points
Explanation: Correct Answer – C and D Gateway VPC Endpoint provides secure private access to Amazon S3 and DynamoDB without traffic routing via the Internet. When Gateway Endpoints are created, VPC Endpoint is created along with the addition of S3 prefixes in the routing table, pointing to VPCE. AWS PrivateLink provides secure private access to various AWS services by adding an Elastic Network Interface within a VPC. AWS creates a local/ regional DNS entry that resolves to the local IP address assigned to ENI. Option A is incorrect as AWS PrivateLink does not support access to Amazon S3. Amazon S3 can be accessed privately from a VPC via Gateway VPC Endpoint. Options B and E are incorrect as with this, the Traffic from EC2 instance to ECS, ECR, and Amazon S3 will be flowing over the Internet. Option F is incorrect as Gateway VPC Endpoint does not support access to Amazon ECR; it supports private access only to Amazon S3 & Amazon DynamoDB.

17-A company is planning to build an application using the services available on AWS. This application will be stateless in nature, and the service must have the ability to scale according to the demand. Which compute service should be used in this scenario?

AWS DynamoDB

AWS Lambda

AWS S3

AWS SQS
0/1 point
Explanation: Correct Answer - B The following content from an AWS Whitepaper supports the usage of AWS Lambda for this requirement: A stateless application is an application that needs no knowledge of previous interactions and stores no session information. Such an example could be an application that, given the same input, provides the same response to any end-user. A stateless application can scale horizontally since any request can be serviced by any of the available compute resources (e.g., EC2 instances, AWS Lambda functions).

18-A large IT company is using Amazon CloudFront for its web application. Static Content for this application is saved in Amazon S3 bucket. Amazon CloudFront is configured for this application to provide faster access to these files for global users. IT Team is concerned for some critical files which needs to be accessed only by users from certain white-list IP pools which you have defined in Amazon CloudFront & no users should be able to access these files directly using Amazon S3 URL. Which of the following is the most secure way controlling access to these files?

Create an OAI user to associate with distribution & modify permission on Amazon S3 bucket using bucket policy.

Create Amazon CloudFront Signed URLs to limit access to these files & modify permission on Amazon S3 bucket using bucket policy.

Create an OAI user to associate with distribution & modify permission on Amazon S3 bucket using object ACL’s.

Create Amazon CloudFront Signed URLs to limit access to these files & modify permission on Amazon S3 bucket using object ACL’s.
0/1 point
Explanation: Correct Answer – C Amazon CloudFront Origin Access Identity is a special user which can be used to control access to content in Amazon S3 bucket. Using Object ACL’s provides a granular control on each file in Amazon S3 bucket. Associating Amazon CloudFront OAI to a distribution & modifying permission on S3 bucket to allow access only to OAI, ensures that no users can directly access content in S3 bucket & all access is pass through Amazon CloudFront using OAI. Option A is incorrect as modifying permission in Amazon S3 bucket using bucket policy will not provide granular control on access to each file in a bucket. Option B & D are incorrect as Amazon CloudFront Signed URLs will provide access only to authorised users for a specified time period, but it will not ensure that this access is through Amazon CloudFront.

19-A start-up firm has a corporate office at New York & regional office in Washington & Chicago. These offices are interconnected over Internet links. Recently they have migrated a few application servers to EC2 instance launched in AWS US-east-1 region. The Developer Team located at the corporate office requires secure access to these servers for initial testing & performance checks before go-live of new application. Since the go-live date is approaching soon, the IT team is looking for quick connectivity to be established. As an AWS consultant which link option will you suggest for a cost effective & quick way to establish secure connectivity from on-premise to servers launched in AWS?

Use AWS Direct Connect to establish IPSEC connectivity from On-premise to VGW.

Use Hardware VPN to establish IPSEC connectivity from On-premise to VGW.

Use Hardware VPN over AWS Direct Connect to establish IPSEC connectivity from On-premise to VGW.

Use Software VPN to establish IPSEC connectivity from On-premise to EC2 instance.
0/1 point
Explanation: Correct Answer – B Using AWS VPN is the fastest & cost-effective way of establishing IPSEC connectivity from on-premise to AWS. Since Internet links are already available, IT teams can quickly setup a VPN connection with VGW in the US-east-1 region. Option A is incorrect as AWS Direct Connect does not provide IPSEC connectivity & it is not a quick way to establish connectivity. Option C is incorrect as Although this will provide a high performance secure IPSEC connectivity from On-premise to AWS, it is not a quick way to establish connectivity. Option D is incorrect as this will incur additional management & cost for maintaining Amazon EC2 instance with required VPN software.

20-. A company plans to deploy a batch processing application in AWS. Which of the followings would ideally help to host this application? (SELECT TWO)

Copy the batch processing application to an ECS Container.

Create a docker image of your batch processing application.

Deploy the image as an Amazon ECS task.

Deploy the container behind the ELB.
1/2 points
Explanation: Correct Answer – B and C AWS Documentation mentions the following: Docker containers are particularly suited for batch job workloads. Batch jobs are often short-lived and embarrassingly parallel. You can package your batch processing application into a Docker image so that you can deploy it anywhere, such as in an Amazon ECS task.
